{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Iterable, Callable, Literal, SupportsFloat, SupportsInt\n",
    "from numpy.typing import NDArray\n",
    "from torch.types import Tensor, Device\n",
    "from pandas import DataFrame\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "from cpscheduler.environment import SchedulingCPEnv, Env, AsyncVectorEnv, VectorEnv, SyncVectorEnv, RayVectorEnv\n",
    "from cpscheduler.environment.instances import generate_taillard_instance, read_jsp_instance\n",
    "from cpscheduler.environment.wrappers import WrappedEnv\n",
    "from cpscheduler.environment.utils import AVAILABLE_SOLVERS, is_iterable_type, convert_to_list\n",
    "from cpscheduler.algorithms import BaseAlgorithm, Buffer, Logs\n",
    "from cpscheduler.common_envs import JobShopEnv\n",
    "\n",
    "root = Path().absolute().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint Programming\n",
    "\n",
    "This notebook is a implementation of the paper _An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling Problems Based on Constraint Programming_, which introduces a scheme for training schedulers for Job-shop instances with different sizes, relying on a Constraint-Programming algorithm to provide the current state of the environment (This idea is brought into this repository to calculate the earliest possible times for future tasks, not for the state itself.).\n",
    "\n",
    "## Introduction\n",
    "The **Job-Shop Scheduling Problem (JSSP)** is a well-known combinatorial optimization challenge where a set of jobs, each consisting of ordered operations, must be scheduled on available machines while minimizing an objective like the makespan or total tardiness. This problem is ubiquitous in industries like manufacturing, logistics, and healthcare, yet solving large-scale instances is computationally challenging due to their NP-hard nature.\n",
    "\n",
    "Traditional Constraint Programming (CP) provides a compact and declarative way to model such problems. While CP solvers are efficient for small instances, they struggle with scaling, necessitating faster heuristic solutions in real-world settings. Priority Dispatching Rules (PDRs), common in these scenarios, are limited by their handcrafted nature and inconsistent performance.\n",
    "\n",
    "In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for learning policies to solve optimization problems. Existing RL approaches for JSSP often depend on custom simulations, handcrafted reward functions, and heavy feature engineering, limiting their generality and scalability.\n",
    "\n",
    "### State Space\n",
    "The state space of the paper is very different from the ones usually employed in RL approaches for JSP. The operations graph is not considered, instead, the interval variables of tasks within an operation range, for each job, are considered, given a set of jobs and the operations $o_{ij}$ put into the state are, for each job,\n",
    "- Previous operation: The last completed operation of the job.\n",
    "- Current operation: The operation currently scheduled (not finished)\n",
    "- Next operations: A fixed number of upcoming operations\n",
    "\n",
    "Each operation is represented as a 4-tuple regarding the operation state in current environment, including\n",
    "- Fixed: Boolean indicating whether the task is fixed or not.\n",
    "- Start Lower bound: The lower bound for the start time of the task.\n",
    "- Processing time: The processing time of the task.\n",
    "- Available: Boolean indicating whether the task is available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_state(\n",
    "        obs: NDArray[np.void],\n",
    "        env: SchedulingCPEnv,\n",
    "        n_future_tasks: int = 3\n",
    "    ) -> Tensor:\n",
    "    jobs = np.unique(obs['job'])\n",
    "\n",
    "    is_fixed    = np.array(env.tasks.is_fixed())\n",
    "    lower_bound = np.array(env.tasks.get_start_lb())\n",
    "\n",
    "    order = np.argsort(obs, order=['job', 'operation'])\n",
    "\n",
    "    obs         = obs[order]\n",
    "    is_fixed    = is_fixed[order]\n",
    "    lower_bound = lower_bound[order]\n",
    "\n",
    "    state      = np.zeros((len(jobs), 2+n_future_tasks, 6), dtype=np.int32)\n",
    "\n",
    "    for i, job in enumerate(jobs):\n",
    "        job_obs = obs[obs['job'] == job]\n",
    "\n",
    "        last_finished = np.max(job_obs['operation'][job_obs['buffer'] == 'finished'], initial=-1)\n",
    "        n_jobs = len(job_obs)\n",
    "\n",
    "        job_ops = [op if (0 <= op < n_jobs) else -1 for op in range(last_finished, last_finished+n_future_tasks+2)]\n",
    "\n",
    "        job_mask = np.array(job_ops) != -1\n",
    "\n",
    "        # Kinda smelly code\n",
    "        state[i, job_mask, 0] = is_fixed[obs['job'] == job][job_ops][job_mask]\n",
    "        state[i, job_mask, 1] = lower_bound[obs['job'] == job][job_ops][job_mask]\n",
    "        state[i, job_mask, 2] = job_obs[job_ops]['processing_time'][job_mask]\n",
    "        state[i, job_mask, 3] = (job_obs[job_ops]['buffer'] == 'available')[job_mask]\n",
    "        state[i, job_mask, 4] = job_obs[job_ops]['machine'][job_mask]\n",
    "        state[i, :, 5] = job_ops\n",
    "\n",
    "    return torch.from_numpy(state)\n",
    "\n",
    "\n",
    "class End2EndEnv(WrappedEnv):\n",
    "    instance_: list[list[int]]\n",
    "    jobs: list[int]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: SchedulingCPEnv,\n",
    "            n_jobs: int,\n",
    "            n_future_tasks: int = 3\n",
    "        ):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.n_jobs = n_jobs\n",
    "        self.n_future_tasks = n_future_tasks\n",
    "\n",
    "        self.instance_   = [[] for _ in range(self.n_jobs)]\n",
    "        self.current_ops = [ 0 for _ in range(self.n_jobs)]\n",
    "\n",
    "\n",
    "    def process_instance(self, obs: dict[str, list[Any]]) -> None:\n",
    "        obs = self.env._get_obs()\n",
    "        self.jobs = obs['job']\n",
    "\n",
    "        # We suppose that the tasks are ordered by operation\n",
    "        task: int\n",
    "        for task in obs['task_id']:\n",
    "            operation: int = obs['operation'][task]\n",
    "            job: int       = obs['job'][task]\n",
    "\n",
    "            if operation >= len(self.instance_[job]):\n",
    "                self.instance_[job].extend([-1 for _ in range(operation - len(self.instance_[job]) + 1)])\n",
    "\n",
    "            self.instance_[job][operation] = task\n",
    "\n",
    "            if obs['buffer'][task] == 'finished' or obs['buffer'][task] == 'executing':\n",
    "                self.current_ops[job] = operation + 1\n",
    "\n",
    "\n",
    "    def reset(self) -> tuple[Tensor, dict[str, Any]]:\n",
    "        obs, info = self.env.reset()\n",
    "        self.process_instance(obs)\n",
    "\n",
    "        obs = np.array(\n",
    "            list(zip(*obs.values())),\n",
    "            dtype=[\n",
    "                ('task_id', np.int32),\n",
    "                ('job', np.int32),\n",
    "                ('operation', np.int32),\n",
    "                ('machine', np.int32),\n",
    "                ('processing_time', np.int32),\n",
    "                ('remaining_time', np.int32),\n",
    "                ('buffer', 'U9')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        new_obs = build_state(obs, self.env, self.n_future_tasks)\n",
    "\n",
    "        return new_obs, info\n",
    "\n",
    "\n",
    "    def step(self, action: SupportsInt | Iterable[SupportsInt]) -> tuple[Tensor, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        if is_iterable_type(action, SupportsInt):\n",
    "            parsed_action = convert_to_list(action, int)\n",
    "\n",
    "        else:\n",
    "            assert isinstance(action, SupportsInt)\n",
    "            parsed_action = [int(action)] if int(action) < self.n_jobs else None\n",
    "\n",
    "\n",
    "        processed_action: Optional[list[int]]\n",
    "        if parsed_action is None:\n",
    "            processed_action = None\n",
    "\n",
    "        else:\n",
    "            processed_action = []\n",
    "            for job_idx in parsed_action:\n",
    "                if job_idx >= self.n_jobs:\n",
    "                    continue\n",
    "\n",
    "                idx = self.current_ops[job_idx]\n",
    "\n",
    "                processed_action.append(self.instance_[job_idx][idx])\n",
    "\n",
    "                self.current_ops[job_idx] += 1\n",
    "\n",
    "\n",
    "        obs, reward, terminated, truncated, info = self.env.step(processed_action)\n",
    "\n",
    "        obs = np.array(\n",
    "            list(zip(*obs.values())),\n",
    "            dtype=[\n",
    "                ('task_id', np.int32),\n",
    "                ('job', np.int32),\n",
    "                ('operation', np.int32),\n",
    "                ('machine', np.int32),\n",
    "                ('processing_time', np.int32),\n",
    "                ('remaining_time', np.int32),\n",
    "                ('buffer', 'U9')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        new_obs = build_state(obs, self.env, self.n_future_tasks)\n",
    "        return new_obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def get_cp_solution(\n",
    "            self,\n",
    "            timelimit: Optional[float] = None,\n",
    "            solver: AVAILABLE_SOLVERS = 'cplex'\n",
    "        ) -> tuple[list[int], list[int], SupportsFloat, bool]:\n",
    "\n",
    "        task_order, start_times, objective_values, is_optimal = self.env.get_cp_solution(timelimit, solver)\n",
    "\n",
    "        parsed_starts = [start_times[task] for task in task_order]\n",
    "        action        = [self.jobs[idx] for idx in task_order]\n",
    "\n",
    "        return action, parsed_starts, objective_values, is_optimal\n",
    "\n",
    "\n",
    "def make_env(\n",
    "        n_jobs: int,\n",
    "        n_machines: int,\n",
    "        n_future_tasks: int = 3\n",
    "    ) -> Callable[[], End2EndEnv]:\n",
    "    def env_fn() -> End2EndEnv:\n",
    "        instance, metadata = generate_taillard_instance(n_jobs, n_machines)\n",
    "\n",
    "        env = JobShopEnv(instance)\n",
    "        env = End2EndEnv(env, metadata['n_jobs'], n_future_tasks)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return env_fn\n",
    "\n",
    "\n",
    "def make_eval_env(\n",
    "        i: int,\n",
    "        n_future_tasks: int = 3\n",
    "    ) -> Callable[[], Env]:\n",
    "    def env_fn() -> Env:\n",
    "        instance, metadata = read_jsp_instance(root / f'instances/jobshop/ta{i:02d}.txt')\n",
    "\n",
    "        env = JobShopEnv(instance)\n",
    "        env = End2EndEnv(env, metadata['n_jobs'], n_future_tasks)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return env_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "The policy used is a 2-stage Transformer that takes the state defined above into the probability of choosen a given job to schedule its operation, or do noting (NO-OP). The first stage is used to learn a representation for the inverval variables, the attention is done between tasks within the same job. The representations are then passed to the second stage, where another transformer takes cross-job information to decide which job to allocate or to do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 100):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, positions: Tensor) -> Tensor:\n",
    "        return self.pe[positions]\n",
    "\n",
    "\n",
    "def layer_init(layer: nn.Module, gain: int = 1, bias_const=0.0) -> nn.Module:\n",
    "    torch.nn.init.orthogonal_(layer.weight, gain)\n",
    "\n",
    "    if hasattr(layer, \"bias\") and layer.bias is not None:\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "class End2EndActor(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model: int,\n",
    "            n_heads: int = 1,\n",
    "            n_layers: int = 1,\n",
    "            dropout: float = 0.\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding_fixed = nn.Embedding(2, 1)\n",
    "        self.embedding_legal_op = nn.Embedding(2, 1)\n",
    "\n",
    "        self.obs_projection = nn.Linear(4, d_model)\n",
    "        layer_init(self.obs_projection)        \n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        transformer_encoder = nn.TransformerEncoderLayer(\n",
    "            d_model, n_heads, dim_feedforward=4*d_model, dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "\n",
    "        action_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, n_heads, dim_feedforward=4*d_model, dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "\n",
    "        self.job_encoder = nn.TransformerEncoder(\n",
    "            transformer_encoder,\n",
    "            n_layers,\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "\n",
    "        self.action_transformer = nn.TransformerEncoder(\n",
    "            action_layer,\n",
    "            n_layers,\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "\n",
    "        self.jobs_action = nn.Sequential(\n",
    "            layer_init(nn.Linear(d_model, out_features=4*d_model)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(4*d_model, 1))\n",
    "        )\n",
    "\n",
    "        self.no_op_action = nn.Sequential(\n",
    "            layer_init(nn.Linear(d_model, 4*d_model)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(4*d_model, 1))\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, obs: Tensor):\n",
    "        embedded_obs = torch.cat((\n",
    "            self.embedding_fixed(obs[:, :, :, 0].long()),\n",
    "            obs[:, :, :, 1:3],\n",
    "            self.embedding_legal_op(obs[:, :, :, 3].long())),\n",
    "        dim=3)\n",
    "\n",
    "        batch_size, n_jobs, n_ops, n_features = embedded_obs.shape\n",
    "\n",
    "        pos_encoding = self.pos_encoder(obs[:, :, :, -1].long())\n",
    "        proj_obs = self.obs_projection(embedded_obs) + pos_encoding\n",
    "\n",
    "        input_mask = obs[:, :, :, -1] == -1\n",
    "\n",
    "        encoded_jobs = self.job_encoder(\n",
    "            proj_obs.view(-1, n_ops, self.d_model),\n",
    "            src_key_padding_mask=input_mask.view(-1, n_ops)\n",
    "        ).view(batch_size, n_jobs, n_ops, self.d_model)\n",
    "\n",
    "        encoded_jobs = encoded_jobs.mean(dim=2)\n",
    "\n",
    "        machines     = obs[:, :, 1, 4].squeeze().float()\n",
    "        finished_job = input_mask[:, :, 1]\n",
    "\n",
    "        job_resource_mask = ~(machines.unsqueeze(1) == machines.unsqueeze(-1))\n",
    "\n",
    "        encodings = self.action_transformer(encoded_jobs, src_key_padding_mask=finished_job, mask=job_resource_mask)\n",
    "\n",
    "        job_final = self.jobs_action(encodings)\n",
    "        no_op     = self.no_op_action(encodings)\n",
    "\n",
    "        logits = torch.cat((job_final.squeeze(2), no_op.mean(dim=1)), dim=1)\n",
    "\n",
    "        available_jobs = obs[:, :, 1, 3].squeeze().bool()\n",
    "\n",
    "        no_op_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=obs.device)\n",
    "        mask = torch.cat((available_jobs, no_op_mask), dim=1)\n",
    "\n",
    "        logits = torch.masked_fill(logits, ~mask, -torch.inf)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class End2End(BaseAlgorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: Device = \"cuda\",\n",
    "        n_envs: int = 128,\n",
    "        n_jobs: int = 10,\n",
    "        n_machines: int = 10,\n",
    "        vector_env: Literal[\"async\", \"sync\", \"ray\"] = \"async\",\n",
    "        *,\n",
    "        buffer_size: int = 100000,\n",
    "        n_future_tasks: int = 3,\n",
    "        clip_coef: float = 0.3,\n",
    "        reward_norm: bool = True,\n",
    "        anneal_lr: bool = True,\n",
    "    ):\n",
    "        buffer_shapes = {\n",
    "            \"obs\": (n_jobs, 2 + n_future_tasks, 6),\n",
    "            \"action\": (),\n",
    "            \"log_prob\": (),\n",
    "            \"returns\": (),\n",
    "        }\n",
    "\n",
    "        buffer = Buffer(buffer_size, buffer_shapes, device)\n",
    "\n",
    "        super().__init__(buffer)\n",
    "\n",
    "        self.agent = agent\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.n_future_tasks = n_future_tasks\n",
    "        self.clip_coef = clip_coef\n",
    "        self.reward_norm = reward_norm\n",
    "        self.anneal_lr = anneal_lr\n",
    "\n",
    "        self.n_jobs     = n_jobs\n",
    "        self.n_machines = n_machines\n",
    "        self.env_fn = make_env(n_jobs, n_machines, n_future_tasks)\n",
    "\n",
    "        self.n_envs = n_envs\n",
    "        self.vector_env = vector_env\n",
    "\n",
    "\n",
    "    def on_session_start(\n",
    "        self, num_updates: int, steps_per_update: int, batch_size: int\n",
    "    ):\n",
    "        if self.anneal_lr:\n",
    "            self.lr_delta = self.optimizer.param_groups[0][\"lr\"] / num_updates\n",
    "\n",
    "\n",
    "    def on_epoch_start(self) -> Logs:\n",
    "        self.agent.eval()\n",
    "        self.buffer.clear()\n",
    "\n",
    "        if self.vector_env == \"async\":\n",
    "            envs = AsyncVectorEnv(\n",
    "                [self.env_fn for _ in range(self.n_envs)], auto_reset=False\n",
    "            )\n",
    "\n",
    "        elif self.vector_env == \"sync\":\n",
    "            envs = SyncVectorEnv(\n",
    "                [self.env_fn for _ in range(self.n_envs)], auto_reset=False\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            envs = RayVectorEnv(\n",
    "                [self.env_fn for _ in range(self.n_envs)], auto_reset=False\n",
    "            )\n",
    "\n",
    "        max_horizon = self.buffer.capacity // self.n_envs\n",
    "\n",
    "        obs, info = envs.reset()\n",
    "\n",
    "        observations = torch.empty(\n",
    "            (self.n_envs, max_horizon, *self.buffer.buffer_shapes[\"obs\"]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        actions = torch.empty(\n",
    "            (self.n_envs, max_horizon, *self.buffer.buffer_shapes[\"action\"]),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        log_probs = torch.empty(\n",
    "            (self.n_envs, max_horizon, *self.buffer.buffer_shapes[\"log_prob\"]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        running_mask = torch.zeros((self.n_envs, max_horizon), dtype=torch.bool)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        no_op = torch.zeros(self.n_envs, dtype=torch.int32)\n",
    "        running = torch.ones(self.n_envs, dtype=torch.bool)\n",
    "        while torch.any(running):\n",
    "            tensor_obs = torch.stack(obs).to(self.device)\n",
    "\n",
    "            running_mask[:, i] = running\n",
    "            observations[:, i] = tensor_obs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.agent(tensor_obs)\n",
    "\n",
    "            categorical = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "            action = categorical.sample()\n",
    "            log_prob = categorical.log_prob(action)\n",
    "\n",
    "            actions[:, i]   = action\n",
    "            log_probs[:, i] = log_prob\n",
    "\n",
    "            obs, reward, terminated, truncated, info = envs.step(action)\n",
    "\n",
    "            running = ~torch.tensor(terminated, dtype=torch.bool)\n",
    "\n",
    "            no_op += torch.where(running, action.cpu() == self.n_jobs, 0)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i >= max_horizon:\n",
    "                break\n",
    "\n",
    "        returns = torch.tensor(info[\"objective_value\"], dtype=torch.float32)\n",
    "\n",
    "        # Calculate partial solution with the cp solver to calculate improvement\n",
    "        min_ep_length = int(torch.sum(running_mask, dim=1).min().item())\n",
    "        sampled_idx = random.randint(0, min_ep_length)\n",
    "\n",
    "        logged_actions = actions[:, :sampled_idx]\n",
    "\n",
    "        envs.reset()\n",
    "\n",
    "        obs, reward, terminated, truncated, info = envs.step(logged_actions)\n",
    "        cp_actions, cp_start_time, cp_makespan, is_optimal = envs.call(\"get_cp_solution\", timelimit=2)\n",
    "\n",
    "        improvement = torch.clamp(1 - torch.tensor(cp_makespan) / returns, 0)\n",
    "\n",
    "        self.buffer.add(\n",
    "            obs      = observations[running_mask],\n",
    "            action   = actions[running_mask],\n",
    "            log_prob = log_probs[running_mask],\n",
    "            returns  = -improvement.repeat(max_horizon, 1).T[running_mask],\n",
    "        )\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        current_op = [0 for _ in range(self.n_envs)]\n",
    "        running_mask = torch.fill(running_mask, False)\n",
    "        running = torch.ones(self.n_envs, dtype=torch.bool)\n",
    "        while torch.any(running):\n",
    "            tensor_obs = torch.stack(obs).to(self.device)\n",
    "\n",
    "            running_mask[:, i] = running\n",
    "            observations[:, i] = tensor_obs\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.agent(tensor_obs)\n",
    "\n",
    "            categorical = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "            current_time: list[int] = info[\"current_time\"]\n",
    "\n",
    "            action = torch.tensor([self.n_jobs for _ in range(self.n_envs)])\n",
    "            for j in range(self.n_envs):\n",
    "                if current_op[j] < len(cp_actions[j]) and current_time[j] == cp_start_time[current_op[j]]:\n",
    "                    action[j] = cp_actions[j][current_op[j]]\n",
    "                    current_op[j] += 1\n",
    "\n",
    "            log_prob = categorical.log_prob(action)\n",
    "\n",
    "            assert torch.isfinite(log_prob).all()\n",
    "\n",
    "            actions[:, i]   = action\n",
    "            log_probs[:, i] = log_prob\n",
    "\n",
    "            obs, reward, terminated, truncated, info = envs.step(action)\n",
    "\n",
    "            running = ~torch.tensor(terminated, dtype=torch.bool)\n",
    "\n",
    "            no_op += torch.where(running, action == self.n_jobs, 0)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i >= max_horizon:\n",
    "                break\n",
    "\n",
    "        self.buffer.add(\n",
    "            obs      = observations[running_mask],\n",
    "            action   = actions[running_mask],\n",
    "            log_prob = log_probs[running_mask],\n",
    "            returns  = improvement.repeat(max_horizon, 1).T[running_mask],\n",
    "        )\n",
    "\n",
    "        envs.close()\n",
    "\n",
    "        return Logs({\"objective\": returns.tolist(), \"no_op_actions\": no_op.tolist()})\n",
    "\n",
    "    def update(self, batch: TensorDict) -> dict[str, Any]:\n",
    "        logits = self.agent(batch[\"obs\"])\n",
    "\n",
    "        log_prob = torch.distributions.Categorical(logits=logits).log_prob(\n",
    "            batch[\"action\"]\n",
    "        )\n",
    "\n",
    "        log_ratio = log_prob - batch[\"log_prob\"]\n",
    "        ratio = torch.exp(log_ratio)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "            approx_kl = torch.mean((ratio - 1) - log_ratio)\n",
    "\n",
    "        returns = batch[\"returns\"]\n",
    "        if self.reward_norm:\n",
    "            returns = (returns - returns.min()) / (returns.max() - returns.min())\n",
    "\n",
    "        if approx_kl > 1000:\n",
    "            for i, action in enumerate(batch[\"action\"]):\n",
    "                if logits[i, action] < -100:\n",
    "                    print(action, logits[i])\n",
    "                    print(batch[\"log_prob\"][i])\n",
    "                    print(batch[\"obs\"][i, action])\n",
    "\n",
    "            raise ValueError(f\"approx_kl is too high: {approx_kl}\")\n",
    "\n",
    "        pg_loss1 = -returns * ratio\n",
    "        pg_loss2 = -returns * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        pg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return {\n",
    "            \"loss\": pg_loss.item(),\n",
    "            \"approx_kl\": approx_kl.item(),\n",
    "        }\n",
    "\n",
    "    def on_epoch_end(self) -> dict[str, Any]:\n",
    "        epoch_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        if self.anneal_lr:\n",
    "            self.optimizer.param_groups[0][\"lr\"] = max(epoch_lr - self.lr_delta, 1e-6)\n",
    "\n",
    "        return {\"learning rate\": epoch_lr}\n",
    "\n",
    "    def validate(self) -> dict[str, Any] | Logs:\n",
    "        makespans = list(map(\n",
    "            int,\n",
    "            [\n",
    "                read_jsp_instance(root / f\"instances/jobshop/ta{i:02d}.txt\")[1][\"Makespan UB\"]\n",
    "                for i in range(1, 81)\n",
    "            ],\n",
    "        ))\n",
    "\n",
    "        val_makespans = []\n",
    "\n",
    "        for group in range(8):\n",
    "            initial = group * 10 + 1\n",
    "            taillard_envs = AsyncVectorEnv(\n",
    "                [make_eval_env(i) for i in range(initial, initial + 10)],\n",
    "                auto_reset=False,\n",
    "            )\n",
    "\n",
    "            obs, info = taillard_envs.reset()\n",
    "            running = [True] * 10\n",
    "            while any(running):\n",
    "                tensor_obs, tensor_mask = obs\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = self.agent(tensor_obs, tensor_mask)\n",
    "                    action = torch.argmax(logits, dim=1)\n",
    "\n",
    "                obs, reward, terminated, truncated, info = taillard_envs.step(\n",
    "                    action.cpu().numpy()\n",
    "                )\n",
    "\n",
    "                running = [not done for done in terminated]\n",
    "\n",
    "            val_makespans.extend(info[\"objective_value\"])\n",
    "\n",
    "        return Logs(\n",
    "            {\n",
    "                \"optimality gap\": [\n",
    "                    (val - optimal) / optimal\n",
    "                    for val, optimal in zip(val_makespans, makespans)\n",
    "                ]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mEnd2EndActor\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(agent\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/scheduling/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scheduling/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/scheduling/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/scheduling/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "d_model = 8\n",
    "n_future_tasks = 3\n",
    "time_limit = 30\n",
    "\n",
    "device = 'cuda:2'\n",
    "seed: int = 42\n",
    "\n",
    "# Training parameters\n",
    "n_envs: int = 4\n",
    "vector_env: Literal['async', 'sync', 'ray'] = 'sync'\n",
    "\n",
    "learning_rate : float           = 2.5e-4\n",
    "batch_size    : int             = 128\n",
    "clip_coef     : float           = 0.3\n",
    "update_epochs : int             = 16\n",
    "n_updates     : int             = 100\n",
    "target_kl     : Optional[float] = 0.01\n",
    "anneal_lr     : bool            = True\n",
    "reward_norm   : bool            = True\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "agent = End2EndActor(d_model).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = End2End(\n",
    "    agent,\n",
    "    optimizer,\n",
    "    n_future_tasks = n_future_tasks,\n",
    "    device         = device,\n",
    "    n_envs         = n_envs,\n",
    "    vector_env     = vector_env,\n",
    "    buffer_size    = 100000,\n",
    "    clip_coef      = clip_coef,\n",
    "    reward_norm    = reward_norm,\n",
    "    anneal_lr      = anneal_lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m algo\u001b[38;5;241m.\u001b[39mbegin_experiment(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend2end-makespan\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10x10\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     use_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/daniel.gratti/scheduling-cp/cpscheduler/algorithms/base.py:215\u001b[0m, in \u001b[0;36mBaseAlgorithm.learn\u001b[0;34m(self, num_updates, steps_per_update, batch_size, validation_freq, lr_scheduler)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    213\u001b[0m logs\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 215\u001b[0m start_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m logs\u001b[38;5;241m.\u001b[39mlog(start_logs, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_logs(start_logs)\n",
      "Cell \u001b[0;32mIn[4], line 163\u001b[0m, in \u001b[0;36mEnd2End.on_epoch_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m    159\u001b[0m     cp_actions[j][i] \u001b[38;5;28;01mif\u001b[39;00m running[j] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs)\n\u001b[1;32m    160\u001b[0m ], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m categorical\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misfinite(log_prob))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    165\u001b[0m actions[:, i]   \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    166\u001b[0m log_probs[:, i] \u001b[38;5;241m=\u001b[39m log_prob\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "algo.begin_experiment(\n",
    "    'end2end-makespan',\n",
    "    '10x10',\n",
    "    root / 'logs',\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "algo.learn(\n",
    "    n_updates,\n",
    "    update_epochs,\n",
    "    batch_size,\n",
    "    validation_freq=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scheduling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
